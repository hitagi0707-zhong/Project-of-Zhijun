# Project-of-Zhijun
This is a repository that including all research projects and papers of Zhijun Zhong

This is my undergraduate thesis: Automatic text summarization based on pre-trained language model

A simple abstract about the thesis is avaliable:

   In the Internet era of information explosion, with the rise and development of
various media platforms, individuals daily live is filled with endless information. 
How to find the information demanded from the mass information is gradually
challenging. With the increasing need and the development of deep learning 
technology, research on automatic summarization has attracted much attention. 
Automatic summarization technology aims to automatically output a short 
summary by input the article. Research on text summarization technology can help 
people obtain key information from text efficiently and quickly, reduce the time 
cost of information acquisition, and effectively improve reading efficiency. At 
present, most of the abstracts generated by text summarization technology are 
redundant and repetitive. In response to this problem, this paper proposes a novel 
automatic text summarization method based on a pre-trained language model, and 
carries out a series of research work.

   The prior research of extractive summarization, generative summarization, 
and pre-trained models are first stated in this article. The details of the deep learning 
methods used in this paper, such as LSTM networks, attention mechanisms, prior 
knowledge, and beam search theory are also introduced. The methods using deep 
learning technology to build a sequence-to-sequence benchmark model to achieve 
a generative summarization is described. The LCSTS Chinese short text dataset 
applied in this article and Rouge metrics method are also interpreted. In the first 
experiment, the Seq2Seq model was used to obtain preliminary results. Compared 
with the baseline results proposed in the dataset paper, the preliminary results of 
this paper have improved on the Rouge-1 and Rouge-L metrics scores.

   Subsequently, pre-trained model Bert and its technical principle are discussed. 
An innovative text summarization method combining Bert is proposed, and the 
corresponding Seq2Seq framework is constructed. A two-layer BiLSTM network 
is used at the Encoder and the LSTM network serves as the Decoder. After 
extracting the word vector features of the text and summary through the Bert, the 
extracted features are feed to the corresponding Encoder and Decoder for training. 
The experimental results obtained greatly exceed the previous experiments on the 
Rouge-1, Rouge-2 and Rouge-L scores, and the training time greatly reduce. It
indicates the automatic text summarization method with the pre-trained language 
model can effectively improve the quality and readability of the generated summary, 
and reduce the training time and cost, which will effectively help people improve 
reading efficiency and save time.


Keywords: Deep learning; Text summarization; Attention mechanism; Seq2seq; 
Bert

